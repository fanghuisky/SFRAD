## 关于小样本的说明
### k-shot
我们测试了`k={1,2,5,10,16,20,50}`七种情况，其中，当`k={1,2,5,10}`时，我们随机测试了
`20`次，而`k={16,20,50}`时，我们随机测试了`10`次。

### 小样本 low_shot_file 文件夹
文件夹 low_shot_file 中包含三个数据集`{mvtec,ksdd,ksdd2}`所使用的随机样本文件，
这三个文件均是由get_random.py文件生成的。

get_random.py中的参数说明
+ used_number_samples: 使用哪些k-shot
+ seed: 对应k-shot的测试次数 
+ dataset_path: 数据集路径
+ dataset_name: 数据集名字
+ class_name: 数据集中的类别

生成随机文件的格式
```shell
1_shot_seed_1_class_name_1_id
1_shot_seed_1_class_name_2_id
...
1_shot_seed_1_clsss_name_n_id
1_shot_seed_2_class_name_1_id
1_shot_seed_2_class_name_2_id
...
1_shot_seed_2_clsss_name_n_id
...
1_shot_seed_k_class_name_1_id
1_shot_seed_k_class_name_2_id
...
1_shot_seed_k_clsss_name_n_id
2_shot_seed_1_class_name_1_id
...
```

###主程序中关于小样本的参数
+ low_shot: [True,False] 小样本或全样本测试
+ dataset_len: 数据集中不同类别的数量，如mvtec中15类，ksdd中3类，用于标记使用随机样本文件中的哪行数据
+ label_times: 执行的是第几次小样本测试

###获取随机样本的核心代码及其说明
```shell
# run_patchcore.py
if (args.low_shot == True):
    # 获取类别的索引
    sub_label = args._CLASSNAMES.index(dataset_name.split('_')[-1])
    # 获得本次测试所对应小样本文件中的行号
    select_row = args.label_times * args.dataset_len + sub_label
    save_label_ = open("./low_shot_file/{}_select_data.txt".format(args.dataset), "r")
    lines = save_label_.readlines()
    # 读取指定行数据，获得所使用的随机样本数
    args.low_shot_select_data = np.array(lines[select_row].split()).astype(int)
    save_label_.close()
    
# patchcore/patchcore.py
count = 0
if(self.is_low_shot==False):
    for image in data_iterator:
        if isinstance(image, dict):
            image = image["image"]
        features.append(_image_to_features(image))
else:
    # low-shot
    for image in data_iterator:
        # 样本编号在小样本列表中则提取特征
        if (count in self.low_shot_select):
            if isinstance(image, dict):
                image = image["image"]
            features.append(_image_to_features(image))
        count = count + 1
```

### 小样本运行的脚本文件

`configs/`文件夹中存放有不同数据集的配置文件，可通过配置文件修改相应的参数，
配置文件参数优先被设置。注意，务必设置`batch_size=1`。

`--label_times`用来设置执行的次数。
如上所述，当`k=1`时，执行`20`次实验，当`k=2`时，执行`20`次，...，
当`k=50`时，执行`10`次，
因此，对于每个数据集，均执行了`110`次小样本实验
```shell
for k in $(seq 0 109)
do
    env PYTHONPATH=src python run_patchcore.py --config 'ksdd.yml' --label_times $k
done
```

## 关于主程序中的核心参数说明
+ --results_path: 存放结果的文件夹
+ --log_project: 存放项目的文件夹，为上个参数的子文件夹
+ --log_group: 存放每次运行的结果，为上个参数的子文件夹
+ --save_segmentation_images: 是否保存结果图
+ --save_patchcore_model: 是否保存模型文件
+ --layers_to_extract_from: 使用哪层的特征
+ --dataset_path: 数据集路径
+ --sampler_name: 采样方法，默认为BFS采样
+ --test_method: 测试方法，默认为ASOMP方法
+ --distance: 计算特征之间的距离，默认为高斯核函数，可设置为欧式距离

有关小样本的参数详见上小结。
注：如果`test_method`设置为`Similarity_Distance`，
且`sampler_name`设置为`approx_greedy_coreset`，则为PatchCore方法。

## 核心方法说明

### BFS
```python
class Basis_Feature_Sampling(GreedyCoresetSampler):
    def __init__(
        self,
        percentage: float,
        device: torch.device,
        number_of_starting_points: int = 10,
        dimension_to_project_features_to: int = 128,
    ):
        """Approximate Greedy Coreset sampling base class."""
        self.number_of_starting_points = number_of_starting_points
        super().__init__(percentage, device, dimension_to_project_features_to)

    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:

        number_of_starting_points = np.clip(
            self.number_of_starting_points, None, len(features)
        )
        # 随机选择一些特征作为初始特征
        start_points = np.random.choice(
            len(features), number_of_starting_points, replace=False
        ).tolist()
        # 计算所有特征到所选择特征的距离
        approximate_distance_matrix = self._compute_batchwise_differences(
            features, features[start_points]
        )
        approximate_coreset_anchor_distances = torch.mean(
            approximate_distance_matrix, axis=-1
        ).reshape(-1, 1)
        # 用来存储所选基特征的编号
        coreset_indices = []
        num_coreset_samples = int(len(features) * self.percentage)
        with torch.no_grad():
            for _ in tqdm.tqdm(range(num_coreset_samples), desc="Subsampling..."):
                # 如果基特征数小于5，则直接挑选所有特征点中到其最近的基特征的最远距离所对应的特征。
                if(len(coreset_indices)<=5):
                    select_idx = torch.argmax(approximate_coreset_anchor_distances).item()
                else: # 如果基特征数大于5，则选出距离最大的两个，计算其表出分数，将分数较大的特征加入基特征中
                    select_idx = torch.argsort(approximate_coreset_anchor_distances, dim=0, descending=True)[:2,0].cpu().numpy()
                    select_idx = self.compute_importance(features, coreset_indices, select_idx)
                coreset_indices.append(select_idx)
                coreset_select_distance = self._compute_batchwise_differences(
                    features, features[select_idx : select_idx + 1]  # noqa: E203
                )
                approximate_coreset_anchor_distances = torch.cat(
                    [approximate_coreset_anchor_distances, coreset_select_distance],
                    dim=-1,
                )
                approximate_coreset_anchor_distances = torch.min(
                    approximate_coreset_anchor_distances, dim=1
                ).values.reshape(-1, 1)

        return np.array(coreset_indices)

    # Criterion B: 计算非冗余性，获得表出距离最大的特征索引
    def compute_importance(self, features, corest_indices, select_idx):
        gallery = features[corest_indices].cpu().numpy()  # gallery特征集
        query = features[select_idx].cpu().numpy()  # query 特征集
        n_samples, dim = query.shape
        scores = np.zeros(n_samples, dtype=np.float32)
        residual = query.copy()
        index = faiss.IndexFlatIP(dim)
        index.add(gallery)

        for j in range(5):
            sims, nbrs = index.search(residual, k=2)  # 查找和残差内积最大的特征的相似度和索引
            # 把本次的索引并入索引集合
            if (j == 0):
                supp = nbrs[:, 0][:, np.newaxis]
            else:
                supp = np.concatenate([supp, nbrs[:, 0][:, np.newaxis]], axis=1)
            # 使用集合中索引所对应的特征线性表出待测特征，并计算其表出残差。
            for i in range(n_samples):
                c = np.linalg.lstsq(gallery[supp[i], :].T, query[i, :].T, rcond=None)[0]
                residual[i] = query[i, :] - np.matmul(c.T, gallery[supp[i], :])
                if(j==4):
                    scores[i] = np.sqrt(np.sum(residual[i] ** 2))
        return select_idx[np.argmax(scores)]
```
### ASOMP
```python
def anomaly_score_calculation_by_omp(self, gallery, query, n_nonzero=5, thr=1.0e-6):

    gallery /= np.linalg.norm(gallery, axis=1).reshape(-1, 1) # 正则化gallery集
    n_samples, dim = query.shape
    scores = np.zeros(n_samples, dtype=np.float32)
    residual = query.copy()

    index = faiss.IndexFlatIP(dim)
    index.add(gallery)

    for j in range(n_nonzero):
        # 找出与残差最近的特征
        sims, nbrs = index.search(residual, k=2)
        if (j == 0):
            supp = nbrs[:, 0][:, np.newaxis]
        else:
            supp = np.concatenate([supp, nbrs[:, 0][:, np.newaxis]], axis=1)
        for i in range(n_samples):
            # 使用最小二乘法计算表出系数
            c = np.linalg.lstsq(gallery[supp[i], :].T, query[i, :].T, rcond=None)[0]
            residual[i] = query[i, :] - np.matmul(c.T, gallery[supp[i], :])
            if(j==n_nonzero-1):
                if (self.is_low_shot or not self.distance):
                    # 测试小样本 或者 指定其他时，则使用欧式距离
                    scores[i] = np.sqrt(np.sum(residual[i] ** 2))
                else:
                    # 使用高斯核函数计算其表出分数
                    scores[i] = 1 - np.exp(-np.sum(residual[i] ** 2) / (2 * self.sigmma))

    return scores
```